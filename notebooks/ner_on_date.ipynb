{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import transformers\n","from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n","from tqdm import tqdm\n","import re"]},{"cell_type":"markdown","metadata":{},"source":["# import datas and clean text"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T15:53:10.612190Z","iopub.status.busy":"2024-03-20T15:53:10.611474Z","iopub.status.idle":"2024-03-20T15:53:11.051770Z","shell.execute_reply":"2024-03-20T15:53:11.049054Z","shell.execute_reply.started":"2024-03-20T15:53:10.612144Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/data-train-test/train_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["train_df = pd.read_csv(\"../datas/train_data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def remove_newlines(df):\n","    df = df.replace(\"\\n\", '', regex=True)\n","    return df\n","train_df = remove_newlines(train_df)\n","train_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Text batching and apply the NER model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def divide_text_into_batches(text, n_batches):\n","    \"\"\"\n","    Divise the text on n batches.\n","    \"\"\"\n","    batch_size = len(text) // n_batches\n","    return [text[i:i+batch_size] for i in range(0, len(text), batch_size)]\n","\n","def extract_sentences_around_dates(batch, model, tokenizer, context_window):\n","    \"\"\"\n","    extract dates and a context of N tokens before and after the dates in a batch\n","    \"\"\"\n","    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n","    results = ner_pipeline(batch)\n","    dates_and_contexts = []\n","\n","    for result in results:\n","        if result['entity'] == 'B-DATE' or result['entity'] == 'I-DATE':\n","            start, end = result['start'], result['end']\n","            pre_context = batch[max(0, start - context_window*6):start]\n","            post_context = batch[end:end + context_window*6]\n","            \n","            # Extract the the text around the date\n","            \n","            # before the date\n","            sentence_boundaries = re.search(r'([.!?]\\s+)|([.!?]$)', pre_context[::-1])\n","            pre_sentence_boundary = -sentence_boundaries.start(0) if sentence_boundaries else -len(pre_context)\n","            \n","            # after the date\n","            sentence_boundaries = re.search(r'([.!?]\\s+)|([.!?]$)', post_context)\n","            post_sentence_boundary = sentence_boundaries.end(0) if sentence_boundaries else len(post_context)\n","            \n","            full_context = pre_context[pre_sentence_boundary:] + batch[start:end] + post_context[:post_sentence_boundary]\n","            \n","            # Clean and extract words around the date\n","            words_around_date = re.findall(r'\\w+', full_context)\n","            date_index = len(re.findall(r'\\w+', pre_context[pre_sentence_boundary:]))\n","            start_context = max(0, date_index - context_window)\n","            end_context = min(len(words_around_date), date_index + context_window)\n","            \n","            context_sentence = ' '.join(words_around_date[start_context:end_context])\n","            dates_and_contexts.append((batch[start:end], context_sentence))\n","\n","    return dates_and_contexts\n","\n","def extract_dates_with_context_from_long_text(text, model, tokenizer, n_batches, context_window):\n","    \"\"\"\n","    Handles long texts by dividing them into batches and extracting sentences around identified dates.\n","    \"\"\"\n","    batches = divide_text_into_batches(text, n_batches)\n","    all_dates_and_contexts = []\n","\n","    for batch in batches:\n","        batch_dates_and_contexts = extract_sentences_around_dates(batch, model, tokenizer, context_window)\n","        all_dates_and_contexts.extend(batch_dates_and_contexts)\n","\n","    return all_dates_and_contexts\n"]},{"cell_type":"markdown","metadata":{},"source":["# Apply the function to all the rows of the dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_NAME = \"Jean-Baptiste/camembert-ner-with-dates\"\n","model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","N_BATCHES = 15\n","CONTEXT_WINDOW = 50"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_to_append = []\n","\n","for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n","    text = row[\"texte\"]\n","    filename = row[\"filename\"]\n","    \n","    dates_and_contexts = extract_dates_with_context_from_long_text(text, model, tokenizer,\n","                                                                   N_BATCHES, CONTEXT_WINDOW)\n","    \n","    contexts = \" \".join([context for _, context in dates_and_contexts])\n","    \n","    data_to_append.append({\"filename\": filename, \"contexts\": contexts})\n","    \n","results_df = pd.DataFrame(data_to_append, columns=[\"filename\", \"contexts\"])\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
