{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-27T22:12:36.722235Z","iopub.status.busy":"2024-03-27T22:12:36.721922Z","iopub.status.idle":"2024-03-27T22:12:59.043271Z","shell.execute_reply":"2024-03-27T22:12:59.042440Z","shell.execute_reply.started":"2024-03-27T22:12:36.722207Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/SamuelLP/Desktop/git/projet_nlp/.nlp_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/Users/SamuelLP/Desktop/git/projet_nlp/.nlp_venv/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"]}],"source":["import sys\n","sys.path.append(\"../src\")\n","\n","import pandas as pd\n","\n","\n","import torch\n","from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n","\n","from fuzzywuzzy import fuzz\n","from tqdm import tqdm\n","import re\n","from preprocess import Preprocessing\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{},"source":["# import datas and clean text"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:12:59.045220Z","iopub.status.busy":"2024-03-27T22:12:59.044541Z","iopub.status.idle":"2024-03-27T22:12:59.515453Z","shell.execute_reply":"2024-03-27T22:12:59.514653Z","shell.execute_reply.started":"2024-03-27T22:12:59.045193Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(\"../datas/train_data.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:12:59.517819Z","iopub.status.busy":"2024-03-27T22:12:59.517480Z","iopub.status.idle":"2024-03-27T22:12:59.622180Z","shell.execute_reply":"2024-03-27T22:12:59.621183Z","shell.execute_reply.started":"2024-03-27T22:12:59.517793Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>filename</th>\n","      <th>texte</th>\n","      <th>sexe</th>\n","      <th>date_accident</th>\n","      <th>date_consolidation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Agen_100515.txt</td>\n","      <td>Le : 12/11/2019 Cour d’appel d’Agen chambre so...</td>\n","      <td>homme</td>\n","      <td>1991-04-09</td>\n","      <td>n.c.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Agen_1100752.txt</td>\n","      <td>Le : 12/11/2019 Cour d’appel d’Agen chambre ci...</td>\n","      <td>homme</td>\n","      <td>2005-06-10</td>\n","      <td>2010-01-19</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Agen_1613.txt</td>\n","      <td>Le : 12/11/2019 Cour d’appel d’Agen Audience p...</td>\n","      <td>femme</td>\n","      <td>1997-09-26</td>\n","      <td>n.c.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Agen_2118.txt</td>\n","      <td>Le : 12/11/2019 Cour d’appel d’Agen Audience p...</td>\n","      <td>femme</td>\n","      <td>1982-08-07</td>\n","      <td>1982-11-07</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Agen_21229.txt</td>\n","      <td>Le : 12/11/2019 Cour d’appel d’Agen Audience p...</td>\n","      <td>homme</td>\n","      <td>1996-11-26</td>\n","      <td>n.c.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID          filename                                              texte  \\\n","0   0   Agen_100515.txt  Le : 12/11/2019 Cour d’appel d’Agen chambre so...   \n","1   1  Agen_1100752.txt  Le : 12/11/2019 Cour d’appel d’Agen chambre ci...   \n","2   2     Agen_1613.txt  Le : 12/11/2019 Cour d’appel d’Agen Audience p...   \n","3   3     Agen_2118.txt  Le : 12/11/2019 Cour d’appel d’Agen Audience p...   \n","4   4    Agen_21229.txt  Le : 12/11/2019 Cour d’appel d’Agen Audience p...   \n","\n","    sexe date_accident date_consolidation  \n","0  homme    1991-04-09               n.c.  \n","1  homme    2005-06-10         2010-01-19  \n","2  femme    1997-09-26               n.c.  \n","3  femme    1982-08-07         1982-11-07  \n","4  homme    1996-11-26               n.c.  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def remove_newlines(df):\n","    df = df.replace(\"\\n\", '', regex=True)\n","    return df\n","train_df = remove_newlines(train_df)\n","\n","preprocess = Preprocessing(train_df)\n","train_df = preprocess.remove_stopwords()\n","\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Text batching and apply the NER model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:12:59.623955Z","iopub.status.busy":"2024-03-27T22:12:59.623581Z","iopub.status.idle":"2024-03-27T22:12:59.638043Z","shell.execute_reply":"2024-03-27T22:12:59.637061Z","shell.execute_reply.started":"2024-03-27T22:12:59.623922Z"},"trusted":true},"outputs":[],"source":["def divide_text_into_batches(text, n_batches):\n","    \"\"\"\n","    Divise the text on n batches.\n","    \"\"\"\n","    batch_size = len(text) // n_batches\n","    return [text[i:i+batch_size] for i in range(0, len(text), batch_size)]\n","\n","def extract_sentences_around_dates(batch, model, tokenizer, context_window):\n","    \"\"\"\n","    extract dates and a context of N tokens before and after the dates in a batch\n","    \"\"\"\n","    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n","    results = ner_pipeline(batch)\n","    dates_and_contexts = []\n","\n","    for result in results:\n","        if result['entity'] == 'B-DATE' or result['entity'] == 'I-DATE':\n","            start, end = result['start'], result['end']\n","            pre_context = batch[max(0, start - context_window*6):start]\n","            post_context = batch[end:end + context_window*6]\n","            \n","            # Extract the the text around the date\n","            \n","            # before the date\n","            sentence_boundaries = re.search(r'([.!?]\\s+)|([.!?]$)', pre_context[::-1])\n","            pre_sentence_boundary = -sentence_boundaries.start(0) if sentence_boundaries else -len(pre_context)\n","            \n","            # after the date\n","            sentence_boundaries = re.search(r'([.!?]\\s+)|([.!?]$)', post_context)\n","            post_sentence_boundary = sentence_boundaries.end(0) if sentence_boundaries else len(post_context)\n","            \n","            full_context = pre_context[pre_sentence_boundary:] + batch[start:end] + post_context[:post_sentence_boundary]\n","            \n","            # Clean and extract words around the date\n","            words_around_date = re.findall(r'\\w+', full_context)\n","            date_index = len(re.findall(r'\\w+', pre_context[pre_sentence_boundary:]))\n","            start_context = max(0, date_index - context_window)\n","            end_context = min(len(words_around_date), date_index + context_window)\n","            \n","            context_sentence = ' '.join(words_around_date[start_context:end_context])\n","            dates_and_contexts.append((batch[start:end], context_sentence))\n","\n","    return dates_and_contexts\n","\n","def extract_dates_with_context_from_long_text(text, model, tokenizer, n_batches, context_window):\n","    \"\"\"\n","    Handles long texts by dividing them into batches and extracting sentences around identified dates.\n","    \"\"\"\n","    batches = divide_text_into_batches(text, n_batches)\n","    all_dates_and_contexts = []\n","\n","    for batch in batches:\n","        batch_dates_and_contexts = extract_sentences_around_dates(batch, model, tokenizer, context_window)\n","        all_dates_and_contexts.extend(batch_dates_and_contexts)\n","\n","    return all_dates_and_contexts\n"]},{"cell_type":"markdown","metadata":{},"source":["# Apply the function to all the rows of the dataframe"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:12:59.640033Z","iopub.status.busy":"2024-03-27T22:12:59.639268Z","iopub.status.idle":"2024-03-27T22:13:03.749723Z","shell.execute_reply":"2024-03-27T22:13:03.748671Z","shell.execute_reply.started":"2024-03-27T22:12:59.639977Z"},"trusted":true},"outputs":[],"source":["MODEL_NAME = \"Jean-Baptiste/camembert-ner-with-dates\"\n","model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","N_BATCHES = 15\n","CONTEXT_WINDOW = 50"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:13:03.751298Z","iopub.status.busy":"2024-03-27T22:13:03.750993Z","iopub.status.idle":"2024-03-27T23:17:54.148165Z","shell.execute_reply":"2024-03-27T23:17:54.147106Z","shell.execute_reply.started":"2024-03-27T22:13:03.751272Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 770/770 [7:43:30<00:00, 36.12s/it]      \n"]}],"source":["data_to_append = []\n","\n","for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n","    text = row[\"texte\"]\n","    filename = row[\"filename\"]\n","    \n","    dates_and_contexts = extract_dates_with_context_from_long_text(text, model, tokenizer,\n","                                                                   N_BATCHES, CONTEXT_WINDOW)\n","    \n","    for _, context in dates_and_contexts:\n","        data_to_append.append({\"filename\": filename, \"texte\": context})\n","    \n","results_df = pd.DataFrame(data_to_append, columns=[\"filename\", \"texte\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Let's use fuzzy wuzzy to remove the duplicated rows"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T23:17:54.149786Z","iopub.status.busy":"2024-03-27T23:17:54.149449Z","iopub.status.idle":"2024-03-27T23:18:03.324372Z","shell.execute_reply":"2024-03-27T23:18:03.323587Z","shell.execute_reply.started":"2024-03-27T23:17:54.149760Z"},"trusted":true},"outputs":[],"source":["def non_duplicate_rows(df, threshold):\n","    rows_to_keep = []\n","\n","    for i in range(len(df) - 1):\n","        ratio = fuzz.ratio(df.iloc[i]['texte'], df.iloc[i+1]['texte'])\n","        if ratio < threshold:\n","            rows_to_keep.append(i)\n","    \n","    rows_to_keep.append(len(df) - 1)\n","    \n","    return df.iloc[rows_to_keep]\n","\n","THRESHOLD = 95\n","df_fuzz = non_duplicate_rows(results_df, THRESHOLD)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T23:18:03.325812Z","iopub.status.busy":"2024-03-27T23:18:03.325489Z","iopub.status.idle":"2024-03-27T23:18:03.330415Z","shell.execute_reply":"2024-03-27T23:18:03.329447Z","shell.execute_reply.started":"2024-03-27T23:18:03.325786Z"},"trusted":true},"outputs":[],"source":["df_fuzz.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df_fuzz = df_fuzz.groupby('filename')['texte'].apply(' '.join).reset_index()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["df_fuzz.to_csv(\"../datas/passages_dates.csv\", index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4661321,"sourceId":7930436,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
